# RoninNVR Docker Compose - CPU Only (No GPU)
#
# Use this on systems without NVIDIA GPU support.
#
# Usage:
#   docker compose -f docker-compose.cpu.yml up -d                    # Live detection only
#   docker compose -f docker-compose.cpu.yml --profile historical up -d  # Include historical ML
#
# ML Processing Modes:
#   - live-detection: Real-time alerts from live streams (default, ~2-5s latency)
#   - ml-worker: Historical analysis of completed recordings (use --profile historical)

services:
  # PostgreSQL Database
  postgres:
    image: postgres:16-alpine
    container_name: ronin-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-ronin_nvr_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-ronin_pass}
      POSTGRES_DB: ${POSTGRES_DB:-ronin_nvr}
    volumes:
      - ${POSTGRES_DATA_PATH:-postgres_data}:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-ronin_nvr_user} -d ${POSTGRES_DB:-ronin_nvr}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # FastAPI Backend
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ronin-backend
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-ronin_nvr_user}:${POSTGRES_PASSWORD:-ronin_pass}@postgres:5432/${POSTGRES_DB:-ronin_nvr}
      STORAGE_ROOT: /data/storage
      HOST: 0.0.0.0
      PORT: 8000
      DEBUG: ${DEBUG:-false}
      JWT_SECRET_KEY: ${JWT_SECRET_KEY:-change-me-in-production}
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-}
      DEFAULT_ADMIN_PASSWORD: ${DEFAULT_ADMIN_PASSWORD:-}
      CORS_ORIGINS: ${CORS_ORIGINS:-http://localhost,http://localhost:3000}
      ML_ENABLED: "true"
      ML_AUTO_PROCESS: "true"
    volumes:
      - ${STORAGE_PATH:-storage_data}:/data/storage
      - ${ML_MODELS_PATH:-ml_models}:/data/storage/.ml/models
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3

  # React Frontend (Nginx)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: ronin-frontend
    restart: unless-stopped
    depends_on:
      - backend
    ports:
      - "80:80"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ML Worker - Historical Analysis (processes completed recordings)
  # Use --profile historical to enable
  ml-worker:
    profiles:
      - historical
    build:
      context: ./backend
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-ronin_nvr_user}:${POSTGRES_PASSWORD:-ronin_pass}@postgres:5432/${POSTGRES_DB:-ronin_nvr}
      STORAGE_ROOT: /data/storage
      ML_WORKERS: 1
      ML_DEFAULT_FPS: ${ML_DEFAULT_FPS:-2.0}
      ML_CONFIDENCE_THRESHOLD: ${ML_CONFIDENCE_THRESHOLD:-0.5}
      ML_CLASS_FILTER: ${ML_CLASS_FILTER:-person,car,truck,bus,motorcycle,bicycle,dog,cat}
    volumes:
      - ${STORAGE_PATH:-storage_data}:/data/storage
      - ${ML_MODELS_PATH:-ml_models}:/data/storage/.ml/models
    command: ["python", "ml_worker.py", "--workers", "1"]
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  # Live Detection Worker (CPU only, single instance)
  live-detection:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ronin-live-detection
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
      backend:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+asyncpg://${POSTGRES_USER:-ronin_nvr_user}:${POSTGRES_PASSWORD:-ronin_pass}@postgres:5432/${POSTGRES_DB:-ronin_nvr}
      STORAGE_ROOT: /data/storage
      LIVE_DETECTION_ENABLED: ${LIVE_DETECTION_ENABLED:-true}
      LIVE_DETECTION_FPS: ${LIVE_DETECTION_FPS:-0.5}
      LIVE_DETECTION_COOLDOWN: ${LIVE_DETECTION_COOLDOWN:-30.0}
      LIVE_DETECTION_CONFIDENCE: ${LIVE_DETECTION_CONFIDENCE:-0.6}
      LIVE_DETECTION_CLASSES: ${LIVE_DETECTION_CLASSES:-person,car,truck}
      ML_CONFIDENCE_THRESHOLD: ${ML_CONFIDENCE_THRESHOLD:-0.5}
      ML_CLASS_FILTER: ${ML_CLASS_FILTER:-person,car,truck,bus,motorcycle,bicycle,dog,cat}
    volumes:
      - ${STORAGE_PATH:-storage_data}:/data/storage
      - ${ML_MODELS_PATH:-ml_models}:/data/storage/.ml/models
    command: ["python", "live_detection_worker.py"]
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 1G

  # Transcode Worker (CPU only)
  # Uses file-based semaphore to limit concurrent transcodes
  # For CPU-only, default to 1 concurrent (libx265 is CPU-intensive)
  transcode-worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    restart: unless-stopped
    depends_on:
      - backend
    environment:
      STORAGE_ROOT: /data/storage
      TRANSCODE_CRF: ${TRANSCODE_CRF:-28}
      TRANSCODE_PRESET: ${TRANSCODE_PRESET:-medium}
      TRANSCODE_MAX_CONCURRENT: ${TRANSCODE_MAX_CONCURRENT:-1}
    volumes:
      - ${STORAGE_PATH:-storage_data}:/data/storage
    command: >
      python transcode_worker.py --continuous --no-gpu --max-concurrent ${TRANSCODE_MAX_CONCURRENT:-1}
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 2G

volumes:
  postgres_data:
    name: ronin_postgres_data
  storage_data:
    name: ronin_storage_data
  ml_models:
    name: ronin_ml_models

networks:
  default:
    name: ronin_network
